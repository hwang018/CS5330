{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def eucl_dist(a, b, axis=1):\n",
    "    return np.linalg.norm(a - b, axis=axis)\n",
    "\n",
    "def get_1d_projection(x,dim_out=1,scaling=True):\n",
    "    '''\n",
    "    input: \n",
    "    x: original data, in np array\n",
    "    dim_out: dimension of projected dataset,default is 1\n",
    "    scaling: kept as true, scale x before pca\n",
    "    '''\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    pca = PCA(n_components=dim_out)\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "\n",
    "    return principalComponents\n",
    "\n",
    "def get_best_1d(x):\n",
    "    '''\n",
    "    get best point from 1d projection of dataset\n",
    "    output: the original point in dataset x (before pca), maybe multiple points qualified\n",
    "    due to precision isclose, we then randomly pick from the best point set\n",
    "    '''\n",
    "    x_1d = get_1d_projection(x)\n",
    "    x_median = np.mean(x_1d)\n",
    "    \n",
    "    best_pts = []\n",
    "    for each in x_1d:\n",
    "        #test if it's close to median value\n",
    "        temp = np.isclose(each, x_median, rtol=1e-01, atol=1e-01, equal_nan=False)\n",
    "        if temp:\n",
    "            best_pt_ind = np.where(x_1d == each)\n",
    "            #strip away brackets of np array and find best_pt from original x\n",
    "            best_pt = x[best_pt_ind[0][0]]\n",
    "            best_pts.append(best_pt)\n",
    "    #random pick one from best points \n",
    "    return random.choice(best_pts)\n",
    "\n",
    "def get_center(k, X, first_pt_process = True):\n",
    "    '''\n",
    "    added a choice to choose first point more intelligently\n",
    "    added control of this functionality\n",
    "    '''\n",
    "    temp = []\n",
    "    if first_pt_process:\n",
    "        #first center is appended here\n",
    "        first_pt = get_best_1d(X)\n",
    "        temp.append(first_pt)\n",
    "    else:\n",
    "        #original uniform random:\n",
    "        temp.append(X[np.random.randint(0, len(X))])\n",
    "    \n",
    "    while len(temp)<k:\n",
    "        d2 = np.array([min([np.square(eucl_dist(i,c, None)) for c in temp]) for i in X])\n",
    "        prob = d2/d2.sum()\n",
    "        cum_prob = prob.cumsum()\n",
    "        r = np.random.random()\n",
    "        ind = np.where(cum_prob >= r)[0][0]\n",
    "        temp.append(X[ind])\n",
    "    return np.array(temp)\n",
    "\n",
    "def k_mean_pp(x, k,first_pt_process=True):\n",
    "    # error = {key:0.0 for key in range(2,11)}\n",
    "    # initalizing cluster variable\n",
    "    center = get_center(k, x,first_pt_process)\n",
    "    # for k in range(2,11):\n",
    "    # assigining zeros to old centroids value\n",
    "    center_old = np.zeros(center.shape)\n",
    "    # initial error\n",
    "    err = eucl_dist(center, center_old, None)\n",
    "\n",
    "    cluster = {}\n",
    "    \n",
    "    while err != 0:\n",
    "        # calculatin distance of data points from centroids and assiging min distance cluster centroid as data point cluster\n",
    "        for i in range(len(x)):\n",
    "            distances = eucl_dist(x[i], center)\n",
    "            clust = np.argmin(distances)\n",
    "            cluster[i] = clust\n",
    "        # changing old centroids value\n",
    "        center_old = np.copy(center)\n",
    "\n",
    "        # Finding the new centroids by taking the average value\n",
    "        for i in range(k):\n",
    "            points = [x[j] for j in range(len(x)) if cluster[j] == i]\n",
    "            if points:\n",
    "                center[i] = np.mean(points, axis=0)\n",
    "\n",
    "        # calculation difference between new centroid and old centroid values\n",
    "        err = eucl_dist(center, center_old, None)\n",
    "\n",
    "    # calculation total difference between cluster centroids and cluster data points\n",
    "    error = 0\n",
    "    for i in range(k):\n",
    "        d = [eucl_dist(x[j], center[i], None) for j in range(len(x)) if cluster[j] == i]\n",
    "        error += np.sum(d)\n",
    "\n",
    "    # counting data points in all clusters\n",
    "    count = {key: 0.0 for key in range(k)}\n",
    "    for i in range(len(x)):\n",
    "        count[cluster[i]] += 1\n",
    "\n",
    "    # displaying cluster number, average distance between centroids and data points and cluster count\n",
    "    # print(k, error / len(x), count)\n",
    "\n",
    "    return cluster, error / len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_test(test_rounds,k):\n",
    "    '''\n",
    "    test function comparing kmeans++ \n",
    "    with modified first point selection\n",
    "    test_rounds: rounds to test, per point error is used to compare\n",
    "    k is cluster number to try\n",
    "    '''\n",
    "    use_pca_costs = []\n",
    "    no_pca_costs = []\n",
    "\n",
    "    for i in range(test_rounds):\n",
    "        if i%10 == 1:\n",
    "            print('Round: %d/%d'%(i,test_rounds))\n",
    "        res_1,per_pt_cost_1 = k_mean_pp(x,k)\n",
    "        use_pca_costs.append(per_pt_cost_1)\n",
    "        \n",
    "        res_2,per_pt_cost_2 = k_mean_pp(x,k,False)\n",
    "        no_pca_costs.append(per_pt_cost_2)\n",
    "\n",
    "    pca_avg_cost = np.mean(np.array(use_pca_costs))\n",
    "    no_pca_avg_cost = np.mean(np.array(no_pca_costs))\n",
    "    print('average per point cost: with pca, random first point')\n",
    "    print(pca_avg_cost,no_pca_avg_cost)\n",
    "    return pca_avg_cost,no_pca_avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 1/50\n",
      "Round: 11/50\n",
      "Round: 21/50\n",
      "Round: 31/50\n",
      "Round: 41/50\n",
      "average per point cost: with pca, random first point\n",
      "35.380066968300206 34.48160621904993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.380066968300206, 34.48160621904993)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_test(50,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset of form [[data1],[data2], ....]\n",
    "inp = pickle.load(open('test.pickle', 'rb'))\n",
    "x = np.array([i[0] for i in inp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # loading dataset of form [[data1],[data2], ....]\n",
    "    inp = pickle.load(open('test.pickle', 'rb'))\n",
    "    x = np.array([i[0] for i in inp])\n",
    "\n",
    "    # return cluster number for every data\n",
    "    cluster = k_mean_pp(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
